{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b685818-8688-4178-b101-62657d1cbb11",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c854b69-3775-4adf-8040-2edd8f8402ef",
   "metadata": {},
   "source": [
    "# Basics of Natural Language Processing (NLP)\n",
    "# - Text Preprocessing\n",
    "\n",
    "We now move from files and tables to text content. In this notebook, the focus is on:\n",
    "- apply transparent formatting-level cleaning (e.g., lowercasing, text-specific patterns we want to remove),\n",
    "- understanding  the difference between characters, tokens, and sentences,\n",
    "- tokenize texts and inspect basic linguistic annotations,\n",
    "- prepare text data for quantitative methods (e.g., removing punctuation, removing stop words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9546bd7",
   "metadata": {},
   "source": [
    "## 1. Formatting-level Cleaning\n",
    "\n",
    "Many texts are not in a format that is immediately usable for NLP techniques and can also be improved for human reading. Web-scraped text is a typical example: it often contains menus, footers, references, and irregular line breaks.\n",
    "\n",
    "For demonstration, we open the content of the Wikipedia page that we saved in Notebook 8 as plain-text string that we can now lightly preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data\n",
    "with open(\"../Data/wikipedia-article.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011e768",
   "metadata": {},
   "source": [
    "One simple option to automate data cleaning is to use Python’s built-in **regular expressions** module, regex (`re`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b9bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d30bc0",
   "metadata": {},
   "source": [
    "First, we want to convert everything to lowercase using just a string method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97150c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07938ad",
   "metadata": {},
   "source": [
    "Second, we remove numeric citations like [32] or [10] with regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d692961",
   "metadata": {},
   "outputs": [],
   "source": [
    " text = re.sub(r\"\\[\\d+\\]\", \" \", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549d261",
   "metadata": {},
   "source": [
    "Third, we collapse multiple whitespaces into a single space with regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39413d",
   "metadata": {},
   "source": [
    "Another example where we aim to extract dates:\n",
    "- `\\b` word boundary, ensures we start at the beginning of a word\n",
    "- `\\d{1,2}` matches 1 or 2 digit day numbers\n",
    "- `[A-Z][a-z]+` matches any capitalized word\n",
    "\n",
    "Limitation: It is not realistically possible to match all date formats with a single regular expression because dates can appear in a huge variety of formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89827f5-88ee-494b-84ee-3df080a3e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The event happened on 12 March, 5 April, and 23 December.\"\n",
    "matches = re.findall(r\"\\b\\d{1,2} [A-Z][a-z]+\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac6f29-0133-4164-8b66-65e23efe9aca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Exercise 1:** \n",
    "\n",
    "What happens if you enter a typo to one of the month names and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d31b50-4773-4b57-b20a-86250feada44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a typo:\n",
    "text = \"The meeting happened on 12 March, 5 Aprils, and 23 December.\" # Typo entered: Aprils\n",
    "matches = re.findall(r\"\\b\\d{1,2} [A-Z][a-z]+\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c3cbf-dc61-4776-afc3-8f4404b7893c",
   "metadata": {},
   "source": [
    "Solution: It does not check whether the word is a real month name - it just checks the pattern (capitalized word after a number). So 5 Aprils still matches, even though \"Aprils\" is a typo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ecddaa",
   "metadata": {},
   "source": [
    "## 2. NLP Library `spaCy`\n",
    "\n",
    "For any data cleaning where we need to decide which words or units to keep or drop (linguistic cleaning), we use an NLP library. Here, we focus on `spaCy`, but other libraries such as `NLTK` exist. \n",
    "\n",
    "We use these NLP libraries to turn raw text into structured pieces that a computer can work with: words, sentences, and simple linguistic labels (e.g. \"this is a verb\", \"this looks like a name\", \"this is probably not very informative\"). For that, we generally use tokens. **Tokenization** is the process of taking a long string of text and breaking it into smaller units called tokens. The content of the text stays the same, but it becomes organized into pieces that can be counted, filtered, and analyzed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a64158",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U setuptools wheel spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b679c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # download the small english model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1cd37f",
   "metadata": {},
   "source": [
    "`spaCy` processes a string and returns a **`doc` object**. This object holds:\n",
    "\n",
    "- tokens,\n",
    "- sentences,\n",
    "- linguistic annotations like lemma, part of speech, entities.\n",
    "\n",
    "Tokens are a technical representation that supports both quantitative and qualitative analysis.\n",
    "\n",
    "Let us look at one example sentence how that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec642aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love learning Python. Python is great for data analysis and Python is also fun!\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb18cb",
   "metadata": {},
   "source": [
    "We let `spaCy` process the text. That means it creates a `doc` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869a324",
   "metadata": {},
   "source": [
    "Let's investigate the `doc` object. First, we print all the sentences detected (`doc.sents` object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af790652",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(repr(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82894c01",
   "metadata": {},
   "source": [
    "The `token` object comes with many attributes that are essential for most NLP tasks. You can explore the full list in the documentation: https://spacy.io/api/token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b448ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    # token.text  : the original form\n",
    "    # token.lemma_: base form (e.g. \"learning\" -> \"learn\")\n",
    "    # token.pos_  : coarse part-of-speech tag (NOUN, VERB, etc.)\n",
    "    # token.is_stop: True if spaCy thinks it is a stop word\n",
    "    print(f\"{token.text!r:12} {token.lemma_!r:12} {token.pos_:8} {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fc33c",
   "metadata": {},
   "source": [
    "**Tokens** become \"quantitative\" once we start counting, aggregating, or modelling them. In many quantitative workflows (e.g. topic modelling), so-called stop words are removed so that very frequent but low-information words do not dominate the patterns. In addition, punctuation is often stripped, because it usually carries little signal for these frequency-based analyses.\n",
    "\n",
    "Below is a small \"cleaning\" example for a text that contains many words that contain little information.\n",
    "\n",
    "The workflow is:\n",
    "1. Get a doc.\n",
    "2. Decide which tokens to keep (rules/filters).\n",
    "3. Collect cleaned tokens (list) or cleaned text (string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"Well, you know, I just want to say that we are basically here today to kind of talk about how we can, in some way, move forward together, and I think that, at the end of the day, people really just want to feel that things are sort of not going in the wrong direction with our democracy.\"\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with spaCy\n",
    "doc = nlp(statement)\n",
    "\n",
    "# Build a cleaned list of tokens step by step using a for-loop (Notebook 4):\n",
    "clean_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    # Skip punctuation (e.g. \"!\" or \",\")\n",
    "    if token.is_punct:\n",
    "        continue\n",
    "    \n",
    "    # Skip spaces \n",
    "    if token.is_space:\n",
    "        continue\n",
    "    \n",
    "    # Skip stop words (very common words like 'and', 'the', 'is' ...)\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "    \n",
    "    # Keep only alphabetic tokens (no numbers, no mixed tokens like \"Python3\")\n",
    "    if not token.is_alpha:\n",
    "        continue\n",
    "    \n",
    "    # If the token passed all checks, add its lowercase form to the list\n",
    "    clean_tokens.append(token.text.lower())\n",
    "print(clean_tokens)\n",
    "\n",
    "# Turn the cleaned tokens back into a single string\n",
    "clean_text = \" \".join(clean_tokens)\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff6d23",
   "metadata": {},
   "source": [
    "We now switch to a dataset that contains a YouTube transcript from the children's story \"The Story of the Little Mole (Who Knew it Was None of His Business)\" (video: https://www.youtube.com/watch?v=plzwDLnieAk). Speech-to-text transcripts like this are a common use-case where Python can support qualitative research. \n",
    "\n",
    "First, we read the transcript file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3071070",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/the-story-of-the-little-mole_transcript.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    story = f.read()   # read entire file as one string\n",
    "\n",
    "# Quick peek at the first 200 characters\n",
    "print(story[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0126ff",
   "metadata": {},
   "source": [
    "Next, we detect and skip lines that are only timestamps and clean small artifacts (like \"[Music]\") before finally joining everything into one continuous story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a37553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "clean_lines = []\n",
    "\n",
    "for line in story.splitlines():\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue  # skip empty lines\n",
    "\n",
    "    # Skip pure timestamp lines like \"0:02\" or \"12:45\"\n",
    "    if re.fullmatch(r\"\\d{1,2}:\\d{2}\", line):\n",
    "        continue\n",
    "\n",
    "    # Remove simple [Music] or other bracketed tags\n",
    "    line = re.sub(r\"\\[.*?\\]\", \"\", line).strip()\n",
    "\n",
    "    if line:\n",
    "        clean_lines.append(line)\n",
    "\n",
    "# Join all remaining lines into one continuous story\n",
    "clean_story = \" \".join(clean_lines)\n",
    "print(clean_story)\n",
    "\n",
    "# Save text\n",
    "with open(\"../Data/the-story-of-the-little-mole_transcript_clean.txt\", \"w\") as f:\n",
    "    f.write(clean_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df712c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Exercise 2:**\n",
    "\n",
    "1. Turn the cleaned story string (`clean_story`) into a `spaCy` doc.  \n",
    "2. Print all sentences that `spaCy` detects.  \n",
    "3. Then, explore a token attribute you might be interested in (look them up here: https://spacy.io/api/token) and for every token in the text, print its attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89340c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (example):\n",
    "import spacy\n",
    "\n",
    "# 1) spaCy Doc\n",
    "doc = nlp(clean_story)\n",
    "\n",
    "# 1) Turn the cleaned story string into a spaCy Doc\n",
    "doc = nlp(clean_story)\n",
    "\n",
    "# 2) Print all sentences detected by spaCy\n",
    "for sent in doc.sents:\n",
    "    print(repr(sent))\n",
    "\n",
    "# 3) Inspect token attributes for all (non-space, non-punctuation) tokens\n",
    "for token in doc:\n",
    "    # token.text  : the original form\n",
    "    # token.lemma_: base form (e.g. \"learning\" -> \"learn\")\n",
    "    # token.pos_  : coarse part-of-speech tag (NOUN, VERB, etc.)\n",
    "    # token.is_stop: True if spaCy thinks it is a stop word\n",
    "    print(f\"{token.text!r:12} {token.lemma_!r:12} {token.pos_:8} {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0c1ab",
   "metadata": {},
   "source": [
    "The sentences look strange because the transcript did not contain punctuation and sentence-like structure, and `spaCy` relies heavily on sentence-ending punctuation and similar cues to decide where one sentence ends and the next begins. Without those signals, `doc.sents` becomes a mostly arbitrary segmentation, so we shouldn’t interpret these sentences linguistically. The tokenization itself, however, is still reliable because it mainly depends on whitespace and basic rules, so token-level information like `text`, `lemma_`, and `pos_` can still be usefully inspected and worked with.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
