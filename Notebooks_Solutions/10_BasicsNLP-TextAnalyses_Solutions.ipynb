{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b685818-8688-4178-b101-62657d1cbb11",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c854b69-3775-4adf-8040-2edd8f8402ef",
   "metadata": {},
   "source": [
    "# Basics of Natural Language Processing (NLP)\n",
    "# - Analyses in Qualitative Research\n",
    "\n",
    "This notebook gives a first glimpse into how simple NLP procedures can support **qualitative and mixed-methods analyses**. We work with already preprocessed texts and look at:\n",
    "\n",
    "1. A simple keyword-in-context search to support close reading.\n",
    "2. Word frequencies and a word cloud.\n",
    "3. A simple sentiment analysis.\n",
    "4. A rule-based topic grouping that shares the basic idea of topic modeling.\n",
    "5. A small network analysis.\n",
    "\n",
    "Technically, procedures are quantitative when texts are turned into numerical representations (for example counts, scores, or topic proportions) so that algorithms can detect patterns. We can treat these patterns as findings in their own right (e.g. overall sentiment trends), or mainly us them as navigation aids, for example, to get an overview, compare groups, and identify interesting passages for close, interpretive reading.\n",
    "\n",
    "The examples in this notebook are intentionally small so that the code and outputs remain readable. In real projects, the same ideas can be scaled up to much larger text collections. Likewise, this notebook can only provide short, illustrative snapshots of each method. Each approach can be configured, validated, and combined in far more systematic and exhaustive ways than we can show here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a661346c",
   "metadata": {},
   "source": [
    "## 1. Close Reading Based on Computational Hints\n",
    "\n",
    "Similar to the project by [The Pudding](https://pudding.cool/2025/07/kids-books/), where the team manually read and coded children’s books, we now want to close-read text passages in which animals occur. Here, we use Python to query their annotations and, in our case, to automatically **extract passages around words**, specifically animal terms. This helps us navigate the stories and identify segments for close, qualitative reading.\n",
    "\n",
    "We illustrate this with the little mole story that we already lightly preprocessed in Notebook 9. \n",
    "\n",
    "First, we define a function for a simple **KWIC (Key Word In Context) search**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8917405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwic_terms(text, terms, window=60):\n",
    "    \"\"\"\n",
    "    For each term in `terms`, print the term with some left and right context.\n",
    "    \"\"\"\n",
    "    # We make a lowercased copy for searching (so \"Mole\" and \"mole\" both match)\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    for term in terms:\n",
    "        term_lower = term.lower()\n",
    "\n",
    "        print(f\"KWIC for term: '{term}'\")\n",
    "\n",
    "        # Start searching at the beginning of the text\n",
    "        search_start = 0\n",
    "        hit_number = 1\n",
    "\n",
    "        while True:\n",
    "            # Find the next position of the term\n",
    "            idx = text_lower.find(term_lower, search_start)\n",
    "\n",
    "            # If nothing is found, we are done with this term\n",
    "            if idx == -1:\n",
    "                break\n",
    "\n",
    "            # Define how much context we want left and right of the term\n",
    "            left_start = max(0, idx - window)\n",
    "            right_end = min(len(text), idx + len(term) + window)\n",
    "\n",
    "            # Cut out left context, the term itself, and right context\n",
    "            left = text[left_start:idx]\n",
    "            middle = text[idx:idx + len(term)]\n",
    "            right = text[idx + len(term):right_end]\n",
    "\n",
    "            # Print one KWIC line\n",
    "            print(f\"{hit_number:02d}: ...{left}>>{middle}<<{right}...\")\n",
    "\n",
    "            # Move the search start to after this hit\n",
    "            hit_number += 1\n",
    "            search_start = idx + len(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a7c4b",
   "metadata": {},
   "source": [
    "Next, we load the preprocessed story, define a list of animals that we want to look for in the story and call the function to do the work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of animals we want to look for in the story:\n",
    "animals = [\"mole\", \"horse\", \"goat\", \"cow\", \"flies\", \"dog\"]\n",
    "\n",
    "# Load text:\n",
    "with open(\"../Data/the-story-of-the-little-mole_transcript_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    story = f.read() \n",
    "\n",
    "# Call the function:\n",
    "kwic_terms(story, animals, window=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f5760",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Exercise 1:**\n",
    "\n",
    "Try to make a tiny change to the `kwic_terms` function such as changing the window size that is defined in the function as `window`. Run the code again and look at the output.\n",
    "What is different now?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60bc05-e0cd-4c45-81b6-23ef1e786c41",
   "metadata": {},
   "source": [
    "## 2. Word Frequencies and Word Cloud\n",
    "\n",
    "Word frequencies offer a simple first orientation. For example: which terms appear most often across all news headlines? We use **lemmas** (base forms) instead of raw word forms so that variants such as \"run\" / \"running\" / \"runs\" are counted together.\n",
    "\n",
    "We then visualise the most frequent lemmas in a **word cloud**, where more frequent words are shown in larger font. This highlights recurring topics and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f9e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca9225",
   "metadata": {},
   "source": [
    "First, let's load the text file, select only the first 1000 headlines (for faster code running), and preprocess the text data using simple string methods. We then process the text with `spaCy` and collect lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961c79a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) Read headlines\n",
    "with open(\"../Data/all_headlines.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    headlines = f.readlines()\n",
    "print(\"Total headlines in file:\", len(headlines))\n",
    "\n",
    "# 2) Use only the first N headlines (to reduce runtime)\n",
    "N = 1000\n",
    "headlines_sample = headlines[:N]\n",
    "print(\"Headlines used:\", len(headlines_sample))\n",
    "\n",
    "# 3) Join to a single text, then preprocess:\n",
    "text = \"\".join(headlines_sample)\n",
    "\n",
    "# remove everything in parentheses (e.g. (PHOTOS), (VIDEO), etc.) and extra spaces\n",
    "text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "text = \" \".join(text.split())\n",
    "print(text)\n",
    "\n",
    "# 4) Process with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# 5) Collect lemmas\n",
    "lemmas = []\n",
    "for tok in doc:\n",
    "    if tok.is_stop or tok.is_punct or tok.is_space:\n",
    "        continue\n",
    "    if len(tok.lemma_) < 3: # filter out very short lemmas\n",
    "        continue\n",
    "    lemmas.append(tok.lemma_.lower())\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff12b54",
   "metadata": {},
   "source": [
    "Next, we count word (lemma) frequencies and plot a word cloud with `WordCloud`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Count how often each lemma appears\n",
    "freq = Counter(lemmas)\n",
    "print(freq.most_common(20))\n",
    "\n",
    "# 7) Word cloud from these frequencies\n",
    "wc = WordCloud(width=800, height=400, background_color=\"white\")\n",
    "wc.generate_from_frequencies(freq)\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e652d6",
   "metadata": {},
   "source": [
    "Why do we see \"woman\" and \"women\" after lemmatization? `spaCy` does not always recognise Women as a \"normal\" plural noun in real headlines, so its rule \"plural → singular\" does not always apply. In some cases, \"women\" is tagged as something else (for example a proper noun/label, PROPN), and then `spaCy` keeps the original form as the lemma (women) instead of turning it into woman. The result is that both woman and women appear as lemmas, which is a normal bit of tagging/lemmatization noise in this (not so much preprocessed) headline dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03525d77-2438-4a3a-af63-2702e5487db2",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "To estimate how positive or negative each evaluation comment is, we use a simple off-the-shelf sentiment tool, `TextBlob`. It assigns two scores to each text:\n",
    "\n",
    "- **Polarity:** from −1 (very negative) to +1 (very positive)\n",
    "- **Subjectivity:** from 0 (very objective) to 1 (very subjective)\n",
    "\n",
    "Internally, `TextBlob` relies on a **lexicon**: many words have predefined sentiment values, and the text’s overall score is an aggregation across these word-level values. More modern transformer-based models (for example BERT variants) can be more accurate, but are heavier to run. For now, `TextBlob` is enough to illustrate the idea.\n",
    "\n",
    "Sentiment analysis can help us find very positive or very negative comments that we may want to read more closely, or compare the overall tone between groups. It is not a precise measure of \"true\" attitudes, because domain-specific language and mixed feelings are easy to misclassify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741528a-97e9-49fd-9c00-0842e00a6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb0bba-662d-4c29-b9f4-ec3d32322f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Python is amazing. But sometimes debugging makes me sad.\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dffd79-4991-4d22-a245-80967517667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Find all .txt files in the folder\n",
    "files = glob.glob(\"../Data/evaluation_comments/*.txt\")\n",
    "\n",
    "# 2) Create an empty list to store the results - one entry for each file\n",
    "rows = []\n",
    "\n",
    "# 3) Loop over all file paths\n",
    "for filepath in files:\n",
    "    # Open each file and read the text\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "\n",
    "    # 4) Compute the sentiment polarity for this text\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "\n",
    "    # 5) Store the result in a dictionary\n",
    "    rows.append({\n",
    "        \"file\": filepath,   # which file did this come from?\n",
    "        \"text\": text,       # the full text\n",
    "        \"polarity\": polarity\n",
    "    })\n",
    "\n",
    "# 6) Turn the list of dictionaries into a DataFrame\n",
    "df_sent = pd.DataFrame(rows)\n",
    "\n",
    "# 7) Print results\n",
    "print(df_sent.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ab22a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Exercise 2:**\n",
    "\n",
    "Use `TextBlob` to analyse the sentiment of your own short text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c4787",
   "metadata": {},
   "source": [
    "## 4. Exploring Topics with Simple Rule-based Grouping\n",
    "\n",
    "In this section we move from individual words to small **themes** or \"topics\". Instead of training a machine-learning model, we use a **simple rule-based grouping**: we define a few keyword lists (for example, words related to \"friendship\" or to \"adventure\") and assign each book description from the children's books collection to the topic whose keywords appear most often. This mimics very simple, dictionary-based coding and shares the **basic idea of topic modeling: grouping texts into topics based on the words they contain.**\n",
    "\n",
    "In more advanced applications, researchers often use machine-learning approaches such as Latent Dirichlet Allocation (LDA) or transformer-based models to detect topics automatically. These models learn groups of words that tend to co-occur and represent texts as mixtures of such topics. In Python, such models are often implemented with packages like `gensim`, `scikit-learn`, or `BERTopic`. However, they require some mathematical and modeling background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4b909-9dc4-4a3b-a43e-25966bc039be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load the data\n",
    "df = pd.read_csv(\"../Data/kids-book-animals.csv\")\n",
    "\n",
    "# 2) Light preprocessing\n",
    "df = df.drop_duplicates(subset=\"description\").reset_index(drop=True) # Only keep each description once\n",
    "df[\"description\"] = df[\"description\"].astype(str) # Make sure the description is a string\n",
    "df[\"description_clean\"] = df[\"description\"].str.lower() # Make lowercase\n",
    "\n",
    "# 3) Define simple word lists for two topics\n",
    "friendship_words = [\"friend\", \"friends\", \"together\", \"share\", \"help\"]\n",
    "adventure_words = [\"journey\", \"adventure\", \"forest\", \"explore\", \"trip\"]\n",
    "\n",
    "# 4) Define a rule-based function\n",
    "def rule_topic(text):\n",
    "    \"\"\"\n",
    "    Simple rule-based topic assignment:\n",
    "    - count how often friendship/adventure words appear\n",
    "    - assign the topic with more matches\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "\n",
    "    friend_count = 0\n",
    "    adventure_count = 0\n",
    "\n",
    "    # Count how many friendship and adventure words appear in this text\n",
    "    for w in words:\n",
    "        if w in friendship_words:\n",
    "            friend_count += 1\n",
    "        if w in adventure_words:\n",
    "            adventure_count += 1\n",
    "\n",
    "    # Decide which topic \"wins\"\n",
    "    if friend_count > adventure_count:\n",
    "        return \"friendship\"\n",
    "    elif adventure_count > friend_count:\n",
    "        return \"adventure\"\n",
    "    else:\n",
    "        return \"unclear\"\n",
    "\n",
    "# 5) Apply the rule-based function to each description\n",
    "df[\"rule_topic\"] = df[\"description_clean\"].apply(rule_topic)\n",
    "\n",
    "# 6) Look at the results\n",
    "print(df[[\"description\", \"rule_topic\"]])\n",
    "\n",
    "# 7) Print first description that was not assigned to a topic\n",
    "print(df.loc[0, \"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d81e3b-cfbe-4a06-acea-c84a80c4dea8",
   "metadata": {},
   "source": [
    "We see limitations of the simple rule-based approach. For example, description number 282 starts with \"Friendship is hard for Fluffy …\", but our rule-based function assigns it the topic \"unclear\". \n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 3:**  \n",
    "Take a moment to think about why this happens.\n",
    "\n",
    "- Which exact words does our code look for in `friendship_words`?\n",
    "- What preprocessing step could help here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37b6a5-4b51-4f7b-be87-43d9eade46d8",
   "metadata": {},
   "source": [
    "Some answers:\n",
    "- This is because our `friendship_words` list only contains exact forms like \"friend\" and \"friends\", but not \"friendship\". The computer only counts exact string matches.\n",
    "- The issue illustrates how strongly our results depend on both preprocessing (for example lowercasing, lemmatization, tokenization) and on the design of our keyword lists. In NLP projects, it is therefore important to look at such outputs carefully and consider how preprocessing steps and methods interact, and where they might need to be refined. The same applies when machine-learning methods are used: these models combine many signals from the text, so their decisions are harder to trace back to individual words or preprocessing steps.\n",
    "- Here, we could extend the dictionaries (e.g. add \"friendship\"), or use lemmatization or simple patterns (e.g. treating all words starting with \"friend\" as friendship-related)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a81652-6265-41e2-a6aa-d431b9785c02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1727904",
   "metadata": {},
   "source": [
    "## 5. Network Analysis\n",
    "\n",
    "We build a small network with `networkx` that links animal labels to pronouns to see which animals are associated with which genders or neutral references in the results of [The Pudding](https://pudding.cool/2025/07/kids-books/). Their results table holds two columns \"animal\" and \"pronoun\". This illustrates the same ideas as larger network analyses (nodes, edges, edge weights) in a very simple setting.\n",
    "\n",
    "In this toy example, animals and pronouns are **nodes**, and we draw an **edge** whenever a pronoun is used for an animal in our dataset. This lets us see, for instance:\n",
    "\n",
    "- which pronouns act as **hubs** (used for many animals),\n",
    "- which animals are linked to several pronouns,\n",
    "- which animals share the same pronoun patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00922b-fe44-48d7-a194-72dca5303f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fb8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# 1) Load the data\n",
    "df = pd.read_csv(\"../Data/kids-book-animals.csv\")\n",
    "\n",
    "# 2) Keep only the most common animals (for example: the 10 most frequent ones)\n",
    "animal_counts = df[\"animal\"].value_counts()\n",
    "common_animals = animal_counts.head(10).index\n",
    "df_small = df[df[\"animal\"].isin(common_animals)].copy()\n",
    "\n",
    "# 3) Build a list of relationships (edges) between animals and pronouns\n",
    "#    Each edge is a pair: [animal, pronoun]\n",
    "edges = []\n",
    "for _, row in df_small.iterrows():\n",
    "    animal = row[\"animal\"]\n",
    "    pronoun = row[\"pronoun\"]\n",
    "    edges.append([animal, pronoun])\n",
    "\n",
    "# 4) Create a NetworkX graph and add all edges at once\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# 5) Draw the network\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw_networkx(G)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
