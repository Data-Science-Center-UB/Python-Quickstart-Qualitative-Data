{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b685818-8688-4178-b101-62657d1cbb11",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c854b69-3775-4adf-8040-2edd8f8402ef",
   "metadata": {},
   "source": [
    "# Basics of Natural Language Processing (NLP)\n",
    "# - Loading Files & Data Organization\n",
    "\n",
    "Natural Language Processing (NLP) is the automatic processing of natural language. In qualitative research, we can treat Python as a \"helper\" for human, interpretive, qualitative work. The goal of this notebook is to show how Python can support reading, coding, and interpretation of text data. These notebooks for NLP are inspired by an open Python textbooks for digital humanities available at [python-textbook.pythonhumanities.com](python-textbook.pythonhumanities.com).\n",
    "\n",
    "---\n",
    "\n",
    "This first notebook demonstrates **how to load files and organise text together with basic metadata**. Throughout this notebook we use a small set of Python libraries. We use `json` and `pandas` to organise texts and metadata in tables, and `BeautifulSoup` to clean web-based texts by stripping HTML.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f6c5e",
   "metadata": {},
   "source": [
    "## 1. The `with` statement\n",
    "\n",
    "Up until now, we’ve worked only with data created inside our own code. But in real-world projects, you’ll mainly work with existing data and files. Let's see how to open and read text files, as well as how to save (write) new files.\n",
    "\n",
    "We can use the `with` operator **to open a text file** and stores it's contents in the variable `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555eed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To open a file (from a relative path) in read mode (\"r\"):\n",
    "with open(\"../Data/kids-book-animals.csv\", \"r\") as f: \n",
    "    data = f.read()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f4cae",
   "metadata": {},
   "source": [
    "**To save text** into a new file, we use almost the same structure, but change the mode to `\"w\"` (`write`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string = \"This is some text.\"\n",
    "\n",
    "# To save data to a file (in a relative path) using write mode (\"w\"):\n",
    "with open(\"../Data/some-text.txt\", \"w\") as f:\n",
    "    f.write(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2cbe84",
   "metadata": {},
   "source": [
    "While the built-in Python `with` statement is sufficient for opening and saving text files, working with larger collections of texts or texts that require some form of structure is easier if we use a few dedicated libraries. The most important Python libraries for handling text data in this notebook are introduced below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430687fe",
   "metadata": {},
   "source": [
    "## 2. `json`\n",
    "\n",
    "The `json` library helps us convert between **JSON text** (as it is stored in a file) and **Python objects** such as dictionaries and lists (compare Notebook 3). JSON (JavaScript Object Notation) is a common way to store and send data on the web, especially when working with websites and APIs.\n",
    "\n",
    "In the dataset we use here, each **line** of the file is one news article stored as a small JSON object. With `json`, we can read the file line by line and turn each JSON line into a normal Python dictionary. Below, we load just a few entries from a real-world dataset of HuffPost news articles (2012–2022), where each line is a separate article in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bcb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the json library (part of Python’s standard library)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68653c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset file (relative path from current notebook)\n",
    "file_path = \"../Data/sample_articles_10000.json\"\n",
    "\n",
    "# Create an empty list to store the loaded articles\n",
    "news_articles = []\n",
    "\n",
    "# Open the file in read mode (\"r\")\n",
    "with open(file_path, \"r\") as f:\n",
    "    # Loop over each line in the file (each line is a separate news article in JSON format)\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:  # Stop after reading the first 5 articles (just for demonstration)\n",
    "            break\n",
    "        # Convert the JSON string (one line) into a Python dictionary and add it to the list\n",
    "        news_articles.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336634dc",
   "metadata": {},
   "source": [
    "To print the category of the first article: take the first item in the list `news_articles` (a dictionary) and access its `category` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_articles[0][\"category\"]) # print category of first article by accessing the first item in the list \"news_articles\" which is a dictionary and then calling its key \"category\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a12b05",
   "metadata": {},
   "source": [
    "## 3. `glob`\n",
    "\n",
    "Suppose you have a folder with multiple .txt files - each one is a transcript of a different interview. You want to automatically load all these files to analyze them in Python. The `glob` library allows you to search for files in a folder based on patterns. In this example, we’ll load all .txt files from a folder and print their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the glob library (part of Python’s standard library)\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e090a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all text files in the \"evaluation_comments\" folder\n",
    "files = glob.glob(\"../Data/evaluation_comments/*.txt\")\n",
    "\n",
    "# Read and display contents of each file\n",
    "for filepath in files:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "        print(f\"--- Contents of {filepath} ---\")\n",
    "        print(content)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb15699",
   "metadata": {},
   "source": [
    "## 4. `beautifulsoup4` & `requests`\n",
    "\n",
    "`beautifulsoup4` is a library used to parse HTML (HyperText Markup Language) and extract information. It’s perfect for getting data from websites in a structured, readable way. Together with `requests` we can conduct our first web scraping task. Let`s scrape some text from a Wikipedia page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afa53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install & import the libraries\n",
    "!pip install beautifulsoup4 \n",
    "!pip install requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42cf66",
   "metadata": {},
   "source": [
    "---\n",
    "### **Exercise 1:** \n",
    "\n",
    "Define a variable that contains the web address (URL) of a Wikipedia page you want to scrape, and then use it in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia page we want to fetch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia asks scripts to send a short \"User-Agent\" string\n",
    "headers = {\n",
    "    \"User-Agent\": \"PythonWorkshop\"\n",
    "}\n",
    "\n",
    "# Download the page\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Turn the HTML into a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27803d7b",
   "metadata": {},
   "source": [
    "The direct output of `soup` is the whole HTML page, which is not very easy to read or work with.\n",
    "If you open this HTML in a text editor, you can see how the page is structured with different tags, for example `<p>` for paragraphs. With `BeautifulSoup` we can select only the parts we care about. Here, we now want to find all `<p>` tags (paragraphs), use a list comprehension (see Notebook 7) to turn them into plain text, use `\" \".join(...)` to combine all paragraphs into one long string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all <p> tags (paragraph elements) on the page\n",
    "paragraph_tags = soup.find_all(\"p\")\n",
    "\n",
    "# Turn each <p> tag into plain text and skip empty paragraphs using list comprehension\n",
    "paragraphs = [p.text.strip() for p in paragraph_tags if p.text.strip()]\n",
    "\n",
    "# Join all paragraphs into one large text block\n",
    "# \" \".join(...) combines a list of strings into one string, with spaces in between\n",
    "main_text = \" \".join(paragraphs)\n",
    "\n",
    "# Print the first 500 characters of the article text\n",
    "print(main_text[:500])\n",
    "\n",
    "# Save to data folder (will be used in Notebook 9)\n",
    "with open(\"../Data/wikipedia-article.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(main_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fa1e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab89df",
   "metadata": {},
   "source": [
    "## 5. `pandas`\n",
    "\n",
    "`pandas` is the most widely used Python library for working with tabular data. Tabular data is data arranged in rows and columns. It is commonly found in files like .csv or Excel spreadsheets. \n",
    "\n",
    "`pandas` makes it easy to read files like `.csv` into `DataFrames`, which keep both the data and its structure intact. This allows you to efficiently store, organize, and analyze data within your code. `DataFrames` are similar to Excel spreadsheets or database tables. They have a 2-dimensional data structure and labeled axes (rows and columns). These are indexed for efficient data retrieval.\n",
    "\n",
    "<img src=\"../Images/dataframe.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install & import pandas\n",
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435ea20",
   "metadata": {},
   "source": [
    "We can also use `pandas` to load and save text files, similar to using the `with` statement, but now we assume a **tabular structure** (rows and columns). For example, the file we loaded at the beginning of this notebook is stored as a table, so we normally read it with `pandas` instead of treating it as one long text string.\n",
    "\n",
    "The imported dataset comes from a study that examines how often animal characters in popular children’s books are explicitly given a gender, in order to reveal patterns and possible biases in representation. If you are curious, have a look at the excellent interactive story created by the team from [The Pudding](https://pudding.cool/2025/07/kids-books/) for a deeper exploration of their findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the same file as a table (DataFrame) with pandas\n",
    "kids = pd.read_csv(\"../Data/kids-book-animals.csv\")\n",
    "\n",
    "# Show the first rows of the table: one row per entry, columns = metadata / text fields\n",
    "kids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778d4136",
   "metadata": {},
   "source": [
    "Once the data are in a `DataFrame`, we can already do very basic analyses that can support qualitative work, such as checking how many entries we have, what columns exist, and how often certain values appear in a column. Kind of all operations one wants to do with structured data are possible. A `DataFrame` follows Python’s usual indexing logic, so we can, for example, select a specific column either by its name or its position (index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad021ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows and columns do we have?\n",
    "print(kids.shape)   # e.g. (number_of_rows, number_of_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4458ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What columns exist in this DataFrame?\n",
    "print(kids.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: select one column by its name (e.g. \"animal\") and then print the \"unique\" string entries (animals) in that column\n",
    "animals = kids[\"animal\"]\n",
    "print(animals.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: select the first column by its index position (column 0)\n",
    "# .iloc is a pandas tool for selecting data from a DataFrame using integer positions.",
    "# dataframe.iloc[row_position, column_position]",
    "first_column = kids.iloc[:, 0]\n",
    "print(first_column.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9732165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: how often does each value appear in the \"animal\" column?\n",
    "print(kids[\"animal\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52a3da",
   "metadata": {},
   "source": [
    "---\n",
    "### **Exercise 2:** \n",
    "\n",
    "In Example 2, we selected the **first column** of `kids` by its index position. Now have a look at this example, think about how this works and write a line of code that selects the **first row** of `kids` by its index position instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ccd76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdab6d72",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
